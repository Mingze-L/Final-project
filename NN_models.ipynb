{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOaloP1KANXG7vM/MnLYrFe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mingze-L/Final-project/blob/main/NN_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrCSbzShGjQy",
        "outputId": "83d965eb-8a2b-43b5-e5e9-67f390f910bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# connect to the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set the directory\n",
        "%cd /content/gdrive/MyDrive/FinalProject/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRL1UcR8xZY3",
        "outputId": "8e64310c-1c8c-4eb4-acff-4f7e07096c4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/FinalProject\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "fm_params = pd.read_csv('fm_params.csv')\n",
        "fm_params_1 = pd.read_csv('fm_params_1.csv')\n",
        "fm_params_2 = pd.read_csv('fm_params_2.csv')\n",
        "fm_params_3 = pd.read_csv('fm_params_3.csv')\n",
        "fm_params_4 = pd.read_csv('fm_params_4.csv')\n",
        "fm_params_5 = pd.read_csv('fm_params_5.csv')\n",
        "\n",
        "m_params = pd.read_csv('m_params.csv')\n",
        "m_params_1 = pd.read_csv('m_params_1.csv')\n",
        "m_params_2 = pd.read_csv('m_params_2.csv')\n",
        "m_params_3 = pd.read_csv('m_params_3.csv')\n",
        "m_params_4 = pd.read_csv('m_params_4.csv')\n",
        "m_params_5 = pd.read_csv('m_params_5.csv')\n",
        "\n",
        "t_params = pd.read_csv('t_params.csv')\n",
        "t_params_1 = pd.read_csv('t_params_1.csv')\n",
        "t_params_2 = pd.read_csv('t_params_2.csv')\n",
        "t_params_3 = pd.read_csv('t_params_3.csv')\n",
        "t_params_4 = pd.read_csv('t_params_4.csv')\n",
        "t_params_5 = pd.read_csv('t_params_5.csv')"
      ],
      "metadata": {
        "id": "habPzongHQKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time lag"
      ],
      "metadata": {
        "id": "dmx0rz94HGC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lag 5(change to 1 is using lag 1 )\n",
        "lags = 5\n",
        "for i in range(1, lags + 1):\n",
        "    fm_params['lag_' + str(i)] = fm_params['K2'].shift(i)\n",
        "    fm_params_1['lag_' + str(i)] = fm_params_1['K2'].shift(i)\n",
        "    fm_params_2['lag_' + str(i)] = fm_params_2['K2'].shift(i)\n",
        "    fm_params_3['lag_' + str(i)] = fm_params_3['K2'].shift(i)\n",
        "    fm_params_4['lag_' + str(i)] = fm_params_4['K2'].shift(i)\n",
        "    fm_params_5['lag_' + str(i)] = fm_params_5['K2'].shift(i)\n",
        "\n",
        "    m_params['lag_' + str(i)] = m_params['K2'].shift(i)\n",
        "    m_params_1['lag_' + str(i)] = m_params_1['K2'].shift(i)\n",
        "    m_params_2['lag_' + str(i)] = m_params_2['K2'].shift(i)\n",
        "    m_params_3['lag_' + str(i)] = m_params_3['K2'].shift(i)\n",
        "    m_params_4['lag_' + str(i)] = m_params_4['K2'].shift(i)\n",
        "    m_params_5['lag_' + str(i)] = m_params_5['K2'].shift(i)\n",
        "\n",
        "    t_params['lag_' + str(i)] = t_params['K2'].shift(i)\n",
        "    t_params_1['lag_' + str(i)] = t_params_1['K2'].shift(i)\n",
        "    t_params_2['lag_' + str(i)] = t_params_2['K2'].shift(i)\n",
        "    t_params_3['lag_' + str(i)] = t_params_3['K2'].shift(i)\n",
        "    t_params_4['lag_' + str(i)] = t_params_4['K2'].shift(i)\n",
        "    t_params_5['lag_' + str(i)] = t_params_5['K2'].shift(i)"
      ],
      "metadata": {
        "id": "Pc-TZkjqG9oU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mark for the autoformer\n",
        "df_stamp = fm_params[['Year']][:]\n",
        "df_stamp['Year'] = pd.to_datetime(df_stamp.Year,format = '%Y')\n",
        "\n",
        "def extract_year(date_obj):\n",
        "    return date_obj.year\n",
        "\n",
        "df_stamp['Year'] = df_stamp['Year'].apply(extract_year)\n",
        "df_stamp['Year'] = (df_stamp['Year'].astype(int)-1922) / (2020-1922) - 0.5\n",
        "for i in range(1, lags + 1):\n",
        "  df_stamp['lag_' + str(i)] = df_stamp['Year'].shift(i)"
      ],
      "metadata": {
        "id": "bV_3950-eBMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train set\n",
        "split_index_fm = int(len(fm_params) * 0.8)\n",
        "split_index_m = int(len(m_params) * 0.8)\n",
        "split_index_t = int(len(t_params) * 0.8)\n",
        "\n",
        "fm = fm_params[:split_index_fm]\n",
        "m = m_params[:split_index_m]\n",
        "t = t_params[:split_index_t]\n",
        "\n",
        "split_index_fm_v = int(len(fm) * 0.8)\n",
        "split_index_m_v = int(len(m) * 0.8)\n",
        "split_index_t_v = int(len(t) * 0.8)\n",
        "\n",
        "train_fm = fm[:split_index_fm_v]\n",
        "train_fm_1 = fm_params_1[:split_index_fm_v]\n",
        "train_fm_2 = fm_params_2[:split_index_fm_v]\n",
        "train_fm_3 = fm_params_3[:split_index_fm_v]\n",
        "train_fm_4 = fm_params_4[:split_index_fm_v]\n",
        "train_fm_5 = fm_params_5[:split_index_fm_v]\n",
        "\n",
        "train_m = m[:split_index_m_v]\n",
        "train_m_1 = m_params_1[:split_index_m_v]\n",
        "train_m_2 = m_params_2[:split_index_m_v]\n",
        "train_m_3 = m_params_3[:split_index_m_v]\n",
        "train_m_4 = m_params_4[:split_index_m_v]\n",
        "train_m_5 = m_params_5[:split_index_m_v]\n",
        "\n",
        "train_t = t[:split_index_t_v]\n",
        "train_t_1 = t_params_1[:split_index_t_v]\n",
        "train_t_2 = t_params_2[:split_index_t_v]\n",
        "train_t_3 = t_params_3[:split_index_t_v]\n",
        "train_t_4 = t_params_4[:split_index_t_v]\n",
        "train_t_5 = t_params_5[:split_index_t_v]\n",
        "\n",
        "# Validation set\n",
        "valid_fm = fm[split_index_fm_v:]\n",
        "valid_m = m[split_index_m_v:]\n",
        "valid_t = t[split_index_t_v:]\n",
        "\n",
        "# Test set\n",
        "test_fm = fm_params[split_index_fm:]\n",
        "test_m = m_params[split_index_m:]\n",
        "test_t = t_params[split_index_t:]"
      ],
      "metadata": {
        "id": "k--KxpQPKY86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the mark\n",
        "train_mark = df_stamp[:split_index_fm_v]\n",
        "valid_mark = df_stamp[split_index_fm_v : split_index_fm]\n",
        "test_mark = df_stamp[split_index_fm:]"
      ],
      "metadata": {
        "id": "JTHR3LmdgZiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "Lsiasc6tpQgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM\n",
        "import torch.nn as nn\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, hidden_size, activation, num_layers, drop_prob):\n",
        "        super(LSTM,self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.LSTM(1, hidden_size,num_layers, batch_first = True, dropout=drop_prob if num_layers > 1 else 0.)\n",
        "        self.fc = nn.Linear(hidden_size,1)\n",
        "        self.act = activation()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x,_ = self.rnn(x)\n",
        "        x = self.fc(x[:,-1:,:])\n",
        "        x = self.act(x)\n",
        "        x = x.squeeze(2)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "btyZY_Gt6FMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LTSF-linear\n",
        "class LTSF(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(LTSF, self).__init__()\n",
        "      self.fc = nn.Linear(1,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "      #X(batch_size, 5, 1)\n",
        "\n",
        "      x = self.fc(x.permute(0,2,1)).permute(0,2,1) #(batch_size, 1, 1)\n",
        "      x = x.squeeze(2) #(batch_size, 1)\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "ZY8WT7Vjs4Bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LTSF-Dlinear\n",
        "\n",
        "class moving_avg(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(moving_avg, self).__init__()\n",
        "      self.kernel_size = 3\n",
        "      self.avg = nn.AvgPool1d(kernel_size=3, stride=1, padding = 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "      # padding on the both ends of time series\n",
        "      front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
        "      end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
        "      x = torch.cat([front, x, end], dim=1)\n",
        "      x = self.avg(x.permute(0, 2, 1))\n",
        "      x = x.permute(0, 2, 1)\n",
        "\n",
        "      return x\n",
        "\n",
        "class series_decomp(nn.Module):\n",
        "    \"\"\"\n",
        "    Series decomposition block\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(series_decomp, self).__init__()\n",
        "        self.moving_avg = moving_avg()\n",
        "\n",
        "    def forward(self, x):\n",
        "        moving_mean = self.moving_avg(x)\n",
        "        res = x - moving_mean\n",
        "        return res, moving_mean\n",
        "\n",
        "class LTSF_D(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(LTSF_D, self).__init__()\n",
        "      self.decompsition = series_decomp()\n",
        "\n",
        "      self.Linear_Seasonal = nn.Linear(5,1)\n",
        "      self.Linear_Trend = nn.Linear(5,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "      #X(batch_size, 5, 1)\n",
        "\n",
        "      seasonal_init, trend_init = self.decompsition(x)\n",
        "      seasonal_init, trend_init = seasonal_init.permute(0,2,1), trend_init.permute(0,2,1) #(batch_size, 1, 5)\n",
        "\n",
        "      seasonal_output = self.Linear_Seasonal(seasonal_init) #(batch_size, 1, 1)\n",
        "      trend_output = self.Linear_Trend(trend_init)\n",
        "\n",
        "      x = seasonal_output + trend_output #(batch_size, 1, 1)\n",
        "      x = x.squeeze(2) #(batch_size, 1)\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "yyZ3lXANs-dF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LTSF-Nlinear\n",
        "\n",
        "class LTSF_N(nn.Module):\n",
        "    def __init__(self, activation):\n",
        "      super(LTSF_N, self).__init__()\n",
        "      self.fc = nn.Linear(3,1)\n",
        "      self.act = activation()\n",
        "\n",
        "    def forward(self, x):\n",
        "      #X(batch_size, 5, 1)\n",
        "      seq_last = x[:,-1:,:].detach()\n",
        "      x = x - seq_last\n",
        "\n",
        "      x = self.fc(x.permute(0,2,1)).permute(0,2,1) #(batch_size, 1, 1)\n",
        "      x = x + seq_last\n",
        "      x = self.act(x)\n",
        "      x = x.squeeze(2) #(batch_size, 1)\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "RH1_3o1wtDx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bi-direction LSTM\n",
        "\n",
        "class biLSTM(nn.Module):\n",
        "    def __init__(self, hidden_size, num_layers, drop_prob, activation):\n",
        "        super(biLSTM,self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.LSTM(1, hidden_size, num_layers, batch_first = True,\n",
        "                           bidirectional = True,\n",
        "                           dropout=drop_prob if num_layers > 1 else 0.)\n",
        "        self.fc = nn.Linear(2*hidden_size,1)\n",
        "        self.act = activation()\n",
        "\n",
        "    def forward(self, x):\n",
        "      #X(batch_size, 5, 1)\n",
        "        x,_ = self.rnn(x) #(batch_size, 5, 2*hidden_size)\n",
        "\n",
        "        x = x[:, -1:, :] #(batch_size, 1, 2*hidden_size)\n",
        "        x = self.fc(x) #(batch_size,1,1)\n",
        "        x = self.act(x)\n",
        "        x = x.squeeze(2) #(batch_size, 1)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "41XvTdwHs0N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Autoformer\n",
        "from layers.Embed import DataEmbedding, DataEmbedding_wo_pos\n",
        "from layers.AutoCorrelation import AutoCorrelation, AutoCorrelationLayer\n",
        "from layers.Autoformer_EncDec import Encoder, Decoder, EncoderLayer, DecoderLayer, my_Layernorm, series_decomp\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Autoformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Autoformer is the first method to achieve the series-wise connection,\n",
        "    with inherent O(LlogL) complexity\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, n_heads, d_ff):\n",
        "        super(Autoformer, self).__init__()\n",
        "\n",
        "        # Decomp\n",
        "        self.decomp = series_decomp(3)\n",
        "\n",
        "        # Embedding\n",
        "        # The series-wise connection inherently contains the sequential information.\n",
        "        # Thus, we can discard the position embedding of transformers.\n",
        "        self.enc_embedding = DataEmbedding_wo_pos(1, d_model, 'timeF', 'm', 0.1)\n",
        "        self.dec_embedding = DataEmbedding_wo_pos(1, d_model, 'timeF', 'm', 0.1)\n",
        "\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = Encoder(\n",
        "            [\n",
        "                EncoderLayer(\n",
        "                    AutoCorrelationLayer(\n",
        "                        AutoCorrelation(False, 3, attention_dropout= 0.1,\n",
        "                                        output_attention=False), d_model, n_heads),\n",
        "                    d_model,\n",
        "                    d_ff,\n",
        "                    moving_avg=3,\n",
        "                    dropout=0.1,\n",
        "                    activation='gelu'\n",
        "                ) for l in range(2)\n",
        "            ],\n",
        "            norm_layer=my_Layernorm(d_model)\n",
        "        )\n",
        "        # Decoder\n",
        "        self.decoder = Decoder(\n",
        "            [\n",
        "                DecoderLayer(\n",
        "                    AutoCorrelationLayer(\n",
        "                        AutoCorrelation(True, 3, attention_dropout= 0.1,\n",
        "                                        output_attention=False), d_model, n_heads),\n",
        "                    AutoCorrelationLayer(\n",
        "                        AutoCorrelation(False, 3, attention_dropout= 0.1,\n",
        "                                        output_attention=False), d_model, n_heads),\n",
        "                    d_model,\n",
        "                    1,\n",
        "                    d_ff,\n",
        "                    moving_avg= 3,\n",
        "                    dropout=0.1,\n",
        "                    activation='gelu',\n",
        "                )\n",
        "                for l in range(1)\n",
        "            ],\n",
        "            norm_layer=my_Layernorm(d_model),\n",
        "            projection=nn.Linear(d_model, 1, bias=True)\n",
        "        )\n",
        "        self.fc = nn.Linear(1,1)\n",
        "        self.act = nn.Tanh()\n",
        "\n",
        "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec,\n",
        "                enc_self_mask=None, dec_self_mask=None, dec_enc_mask=None):\n",
        "        # decomp init\n",
        "        mean = torch.mean(x_enc, dim=1).unsqueeze(1).repeat(1, 2, 1) #(batch_size, 1, 1)\n",
        "        zeros = torch.zeros([x_dec.shape[0], 2, x_dec.shape[2]], device=x_enc.device) #(batch_size, 1, 1)\n",
        "        seasonal_init, trend_init = self.decomp(x_enc[:, :3, :]) #(batch_size, 5 ,1)\n",
        "        # decoder input\n",
        "        trend_init = torch.cat([trend_init, mean], dim=1) #(batch_size, 5, 1)\n",
        "        seasonal_init = torch.cat([seasonal_init, zeros], dim=1) #(batch_size, 5, 1)\n",
        "        # enc\n",
        "        enc_out = self.enc_embedding(x_enc, x_mark_enc) #(batch_size, 5, d_model)\n",
        "        enc_out, attns = self.encoder(enc_out, attn_mask=enc_self_mask)\n",
        "        # dec\n",
        "        dec_out = self.dec_embedding(seasonal_init, x_mark_dec) #(batch_size, 5, d_model)\n",
        "        seasonal_part, trend_part = self.decoder(dec_out, enc_out, x_mask=dec_self_mask, cross_mask=dec_enc_mask,\n",
        "                                                 trend=trend_init)\n",
        "        # final\n",
        "        dec_out = trend_part + seasonal_part\n",
        "        dec_out = dec_out[:, -1:, -1:]\n",
        "        dec_out = self.fc(dec_out)\n",
        "        dec_out = self.act(dec_out)\n",
        "\n",
        "        return dec_out"
      ],
      "metadata": {
        "id": "SN8tvbe4tv9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Female"
      ],
      "metadata": {
        "id": "ICOz2ZSLLDXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the dataloader for female (standardize)\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "X_train = train_fm_5.iloc[5:,3:]\n",
        "Y_train = torch.tensor(train_fm_5['K2'][5:].values).to(torch.float32).unsqueeze(1)\n",
        "\n",
        "X_valid = valid_fm.iloc[:,3:]\n",
        "Y_valid = torch.tensor(valid_fm['K2'].values).to(torch.float32).unsqueeze(1)\n",
        "\n",
        "X_test = test_fm.iloc[:,3:]\n",
        "Y_test = torch.tensor(test_fm['K2'].values).to(torch.float32).unsqueeze(1)\n",
        "\n",
        "# unmark the following code for the hyperparameter tuning\n",
        "#X_hyper = fm_params.iloc[5:,5:]\n",
        "#Y_hyper = torch.tensor(fm_params['K2'][5:].values).to(torch.float32).unsqueeze(1)\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create an instance of the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to your training data\n",
        "scaler.fit(X_train)\n",
        "X_train_scaled = torch.tensor(scaler.transform(X_train)).to(torch.float32).unsqueeze(2)\n",
        "X_valid_scaled = torch.tensor(scaler.transform(X_valid)).to(torch.float32).unsqueeze(2)\n",
        "X_test_scaled = torch.tensor(scaler.transform(X_test)).to(torch.float32).unsqueeze(2)\n",
        "#X_hyper_scaled = torch.tensor(scaler.transform(X_hyper)).to(torch.float32).unsqueeze(2)\n",
        "\n",
        "\n",
        "batch_size = 10\n",
        "\n",
        "# The code for autoformer and can be use for the data of different people\n",
        "X_train_mark = torch.tensor(train_mark.iloc[5:,1:].values).to(torch.float32).unsqueeze(2)\n",
        "X_valid_mark = torch.tensor(valid_mark.iloc[:,1:].values).to(torch.float32).unsqueeze(2)\n",
        "X_test_mark = torch.tensor(test_mark.iloc[:,1:].values).to(torch.float32).unsqueeze(2)\n",
        "\n",
        "\n",
        "#train = TensorDataset(X_train_scaled, Y_train)\n",
        "train = TensorDataset(X_train_scaled, Y_train, X_train_mark)\n",
        "train_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "#valid = TensorDataset(X_valid_scaled, Y_valid)\n",
        "valid = TensorDataset(X_valid_scaled, Y_valid, X_valid_mark)\n",
        "valid_loader = DataLoader(valid, batch_size = batch_size, shuffle = False)"
      ],
      "metadata": {
        "id": "JKbp34yULBXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "GUTe8KoSNq6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install skorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wux5EDce6Wn9",
        "outputId": "164323a9-c860-40c9-8211-83ca4146e6de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting skorch\n",
            "  Downloading skorch-0.14.0-py3-none-any.whl (221 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/221.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m163.8/221.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.3/221.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (1.10.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from skorch) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.10/dist-packages (from skorch) (4.65.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.0->skorch) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.0->skorch) (3.2.0)\n",
            "Installing collected packages: skorch\n",
            "Successfully installed skorch-0.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameter tuning\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import torch.optim as optim\n",
        "from skorch import NeuralNetRegressor\n",
        "\n",
        "model = NeuralNetRegressor(Autoformer,\n",
        "    criterion = nn.MSELoss,\n",
        "    optimizer = optim.Adam,\n",
        "    batch_size = 10,\n",
        "    max_epochs = 50,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "param_grid = {\n",
        "    'optimizer__lr': [0.001, 0.01, 0.1],\n",
        "    'module__activation':[nn.ReLU, nn.Tanh]\n",
        "}\n",
        "\n",
        "\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1,cv=5)\n",
        "\n",
        "\n",
        "grid_result = grid.fit(X_hyper_scaled,Y_hyper)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqrRJVV6taxM",
        "outputId": "a2678c6c-f119-4159-c106-50f792a840b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best: -681.110087 using {'module__activation': <class 'torch.nn.modules.activation.ReLU'>, 'optimizer__lr': 0.1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The traing process(with can be used on all the NN models except Autoformer)\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "model = LSTM(10, 3, 0.3, nn.Tanh).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "learning_rate = 0.01\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "valid_loss_min = np.Inf\n",
        "counter = 0\n",
        "\n",
        "n_iters = 5000\n",
        "num_epochs = n_iters / (len(X_train_scaled) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "for epochs in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    for i, (data, labels) in enumerate(train_loader):\n",
        "        counter += 1\n",
        "        data = data.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        if counter % 50 == 0:\n",
        "            val_losses = []\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "              for data, labels in valid_loader:\n",
        "                data = data.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(data)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_losses.append(loss.item())\n",
        "\n",
        "            model.train()\n",
        "            print(\"Epoch: {}/{}...\".format(epochs+1, num_epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
        "\n",
        "            if np.mean(val_losses) < valid_loss_min:\n",
        "                torch.save(model.state_dict(), './state_dict_biLSTM.pt')\n",
        "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
        "                valid_loss_min = np.mean(val_losses)"
      ],
      "metadata": {
        "id": "vnbOH11qeQua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The training process for Autoformer\n",
        "import torch.optim as optim\n",
        "model = Autoformer(256,8,512).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "learning_rate = 0.0001\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "valid_loss_min = np.Inf\n",
        "counter = 0\n",
        "\n",
        "n_iters = 1000\n",
        "num_epochs = n_iters / (len(X_train_scaled) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "for epochs in range(num_epochs):\n",
        "    model.train()\n",
        "    counter += 1\n",
        "    for i, (data, labels, mark) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        labels = labels.float().to(device).unsqueeze(2)\n",
        "\n",
        "        mark = mark.float().to(device)\n",
        "        # decoder input\n",
        "        dec_inp = torch.zeros_like(labels[:, -1:, :]).float()\n",
        "        dec_inp = torch.cat([labels[:, :4, :], dec_inp], dim=1).float().to(device)\n",
        "\n",
        "        outputs = model(data, mark, dec_inp, mark)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    val_losses = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data, labels, mark in valid_loader:\n",
        "                data = data.to(device)\n",
        "                labels = labels.float().unsqueeze(2).to(device)\n",
        "\n",
        "                mark = mark.float().to(device)\n",
        "                # decoder input\n",
        "                dec_inp = torch.zeros_like(labels[:, -1:, :]).float()\n",
        "                dec_inp = torch.cat([labels[:, :4, :], dec_inp], dim=1).float().to(device)\n",
        "\n",
        "                outputs = model(data, mark, dec_inp, mark)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_losses.append(loss.item())\n",
        "\n",
        "    model.train()\n",
        "    print(\"Epoch: {}/{}...\".format(epochs+1, num_epochs),\n",
        "          \"Loss: {:.6f}...\".format(loss.item()),\n",
        "          \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
        "\n",
        "    if np.mean(val_losses) < valid_loss_min:\n",
        "                torch.save(model.state_dict(), './state_dict_autoformer.pt')\n",
        "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
        "                valid_loss_min = np.mean(val_losses)\n",
        "                counter = 0\n",
        "    elif np.mean(val_losses) >= valid_loss_min and counter == 20:\n",
        "      break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkEJKSf8Ys2y",
        "outputId": "2d4662f7-7ad3-4bc0-fd31-3c21c1ba7162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/172... Loss: 0.131253... Val Loss: 0.120781\n",
            "Validation loss decreased (inf --> 0.120781).  Saving model ...\n",
            "Epoch: 2/172... Loss: 0.159781... Val Loss: 0.145391\n",
            "Epoch: 3/172... Loss: 0.132209... Val Loss: 0.127847\n",
            "Epoch: 4/172... Loss: 0.073526... Val Loss: 0.085546\n",
            "Validation loss decreased (0.120781 --> 0.085546).  Saving model ...\n",
            "Epoch: 5/172... Loss: 0.071521... Val Loss: 0.085698\n",
            "Epoch: 6/172... Loss: 0.097469... Val Loss: 0.100675\n",
            "Epoch: 7/172... Loss: 0.117522... Val Loss: 0.101494\n",
            "Epoch: 8/172... Loss: 0.133785... Val Loss: 0.105014\n",
            "Epoch: 9/172... Loss: 0.141538... Val Loss: 0.110679\n",
            "Epoch: 10/172... Loss: 0.138911... Val Loss: 0.101212\n",
            "Epoch: 11/172... Loss: 0.140086... Val Loss: 0.092801\n",
            "Epoch: 12/172... Loss: 0.147709... Val Loss: 0.099080\n",
            "Epoch: 13/172... Loss: 0.171034... Val Loss: 0.124086\n",
            "Epoch: 14/172... Loss: 0.168476... Val Loss: 0.118868\n",
            "Epoch: 15/172... Loss: 0.150327... Val Loss: 0.102330\n",
            "Epoch: 16/172... Loss: 0.156557... Val Loss: 0.108953\n",
            "Epoch: 17/172... Loss: 0.174716... Val Loss: 0.122637\n",
            "Epoch: 18/172... Loss: 0.195593... Val Loss: 0.139099\n",
            "Epoch: 19/172... Loss: 0.192300... Val Loss: 0.138992\n",
            "Epoch: 20/172... Loss: 0.194421... Val Loss: 0.139687\n",
            "Epoch: 21/172... Loss: 0.190012... Val Loss: 0.131808\n",
            "Epoch: 22/172... Loss: 0.189508... Val Loss: 0.129658\n",
            "Epoch: 23/172... Loss: 0.189202... Val Loss: 0.130959\n",
            "Epoch: 24/172... Loss: 0.189503... Val Loss: 0.132016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate the prediciton output(exclude Autofomer)\n",
        "model.load_state_dict(torch.load('./state_dict_biLSTM.pt'))\n",
        "\n",
        "model.eval()\n",
        "X_test = X_test_scaled.to(device)\n",
        "Y_test = Y_test.to(device)\n",
        "output = model(X_test)\n",
        "output = output.detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "y4nAYJGZM1jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this code use to generate the prediction for the training interval(exclude Autoformer)\n",
        "model.load_state_dict(torch.load('./state_dict_biLSTM.pt'))\n",
        "\n",
        "model.eval()\n",
        "X_train = X_train_scaled.to(device)\n",
        "output = model(X_train)\n",
        "output = output.detach().cpu().numpy()\n",
        "Y_train_m = Y_train_m.detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "dkBbvPaeNDb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate the prediction output for Autoformer\n",
        "model.load_state_dict(torch.load('./state_dict_autoformer.pt'))\n",
        "\n",
        "model.eval()\n",
        "X_test = X_test_scaled.to(device)\n",
        "#Y_test = Y_test.to(device)\n",
        "Y_test = Y_test.to(device).unsqueeze(2)\n",
        "mark = X_test_mark.float().to(device)\n",
        "#decoder input\n",
        "dec_inp = torch.zeros_like(Y_test[:, -1:, :]).float()\n",
        "dec_inp = torch.cat([Y_test[:, :4, :], dec_inp], dim=1).float().to(device)\n",
        "\n",
        "output = model(X_test, mark, dec_inp, mark)\n",
        "#output = model(X_test)\n",
        "output = output.squeeze(2).detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "KBpVSta2Y7CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this code use to generate the prediction for the training interval for Autoformer\n",
        "model.load_state_dict(torch.load('./state_dict_autoformer.pt'))\n",
        "model.eval()\n",
        "X_train = X_train_scaled.to(device)\n",
        "Y_train = Y_train.to(device).unsqueeze(2)\n",
        "mark = X_train_mark.float().to(device)\n",
        "dec_inp = torch.zeros_like(Y_train[:, -1:, :]).float()\n",
        "dec_inp = torch.cat([Y_train[:, :4, :], dec_inp], dim=1).float().to(device)\n",
        "\n",
        "output = model(X_train, mark, dec_inp, mark)\n",
        "output = output.squeeze(2).detach().cpu().numpy()\n",
        "Y_train = Y_train.squeeze(2).detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "kZsYfqhC5EE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create prediciton interval (all the NN model follow the same step)"
      ],
      "metadata": {
        "id": "GGF9xHnCMXDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the training error\n",
        "error_var = (1/57) * np.sum((Y_train-output)**2)"
      ],
      "metadata": {
        "id": "O-Og3QNV5cEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# store the prediction output generated from different sample\n",
        "autoformer_fm = pd.DataFrame(output, columns = ['s'])"
      ],
      "metadata": {
        "id": "5VIgs0OpfCCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoformer_fm['s5'] = output"
      ],
      "metadata": {
        "id": "Ku9EGlQv7-yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoformer_fm['average'] = autoformer_fm.mean(axis=1)"
      ],
      "metadata": {
        "id": "qQ_mWIifCVZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bagged estimation for variance\n",
        "var = 0\n",
        "for i in range(6):\n",
        "  var += (autoformer_fm.iloc[:,i].values - autoformer_fm['average'].values[:,]) ** 2\n",
        "\n",
        "autoformer_fm['k_var'] =  1/5 * var"
      ],
      "metadata": {
        "id": "d94npycEDOXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate the output\n",
        "autoformer_fm['lower_bound'] = autoformer_fm['average'].values + 1.96 * np.sqrt(autoformer_fm['k_var'].values + error_var)\n",
        "autoformer_fm['upper_bound'] = autoformer_fm['average'].values - 1.96 * np.sqrt(autoformer_fm['k_var'].values + error_var)"
      ],
      "metadata": {
        "id": "LIBwTIecKamg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoformer_fm.to_csv('autoformer_fm_2.csv', index=False)"
      ],
      "metadata": {
        "id": "bkDW90Am8ELB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Male"
      ],
      "metadata": {
        "id": "o5Vv1vMNGhEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the dataloader for male\n",
        "X_train_m = train_m_5.iloc[5:,3:]\n",
        "Y_train_m = torch.tensor(train_m_5['K2'][5:].values).to(torch.float32).unsqueeze(1)\n",
        "\n",
        "X_valid_m = valid_m.iloc[:,3:]\n",
        "Y_valid_m = torch.tensor(valid_m['K2'].values).to(torch.float32).unsqueeze(1)\n",
        "\n",
        "X_test_m = test_m.iloc[:,3:]\n",
        "Y_test_m = torch.tensor(test_m['K2'].values).to(torch.float32).unsqueeze(1)\n",
        "\n",
        "#X_hyper_m = m_params.iloc[5:,4:]\n",
        "#Y_hyper_m = torch.tensor(m_params['K2'][5:].values).to(torch.float32).unsqueeze(1)"
      ],
      "metadata": {
        "id": "CBFJKwIOGjGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the scaler to your training data\n",
        "scaler.fit(X_train_m)\n",
        "X_train_scaled_m = torch.tensor(scaler.transform(X_train_m)).to(torch.float32).unsqueeze(2)\n",
        "X_valid_scaled_m = torch.tensor(scaler.transform(X_valid_m)).to(torch.float32).unsqueeze(2)\n",
        "X_test_scaled_m = torch.tensor(scaler.transform(X_test_m)).to(torch.float32).unsqueeze(2)\n",
        "#X_hyper_scaled_m = torch.tensor(scaler.transform(X_hyper_m)).to(torch.float32).unsqueeze(2)\n",
        "\n",
        "batch_size = 10\n",
        "\n",
        "\n",
        "#train = TensorDataset(X_train_scaled_m,Y_train_m)\n",
        "train = TensorDataset(X_train_scaled_m, Y_train_m, X_train_mark) # the X_train_mark is same as the mark generate in Female part\n",
        "train_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "#valid = TensorDataset(X_valid_scaled_m,Y_valid_m)\n",
        "valid = TensorDataset(X_valid_scaled_m, Y_valid_m, X_valid_mark) # the X_valid_mark is same as the mark generate in Female part\n",
        "valid_loader = DataLoader(valid, batch_size = batch_size, shuffle = False)"
      ],
      "metadata": {
        "id": "MswnqpmSHAp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameter tuning\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import torch.optim as optim\n",
        "from skorch import NeuralNetRegressor\n",
        "\n",
        "model = NeuralNetRegressor(LTSF_N,\n",
        "    criterion = nn.MSELoss,\n",
        "    optimizer = optim.Adam,\n",
        "    batch_size = 10,\n",
        "    max_epochs = 100,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "param_grid = {\n",
        "    'optimizer__lr': [0.001, 0.01, 0.1],\n",
        "    'module__activation':[nn.ReLU, nn.Tanh]\n",
        "}\n",
        "\n",
        "\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1,cv=5)\n",
        "\n",
        "\n",
        "grid_result = grid.fit(X_hyper_scaled_m,Y_hyper_m)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Plt6__yTHb29",
        "outputId": "3830dac1-1eb6-48fc-af79-de93ce84f5ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best: -5.487098 using {'module__activation': <class 'torch.nn.modules.activation.ReLU'>, 'optimizer__lr': 0.1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The traing process(with can be used on all the NN models except Autoformer)\n",
        "model = biLSTM(10, 3, 0.3, nn.Tanh).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "learning_rate = 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "valid_loss_min = np.Inf\n",
        "counter = 0\n",
        "\n",
        "n_iters = 5000\n",
        "num_epochs = n_iters / (len(X_train_m) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "for epochs in range(num_epochs):\n",
        "    model.train()\n",
        "    for i, (data, labels) in enumerate(train_loader):\n",
        "        counter += 1\n",
        "        data = data.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if counter % 50 == 0:\n",
        "            val_losses = []\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "               for data, labels in valid_loader:\n",
        "                 data = data.to(device)\n",
        "                 labels = labels.to(device)\n",
        "                 outputs = model(data)\n",
        "                 loss = criterion(outputs, labels)\n",
        "                 val_losses.append(loss.item())\n",
        "\n",
        "            model.train()\n",
        "            print(\"Epoch: {}/{}...\".format(epochs+1, num_epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
        "\n",
        "            if np.mean(val_losses) < valid_loss_min:\n",
        "                torch.save(model.state_dict(), './state_dict_biLSTM-m.pt')\n",
        "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
        "                valid_loss_min = np.mean(val_losses)"
      ],
      "metadata": {
        "id": "2zkgc7xKIb5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The training process for Autoformer\n",
        "import torch.optim as optim\n",
        "model = Autoformer(256,4,128).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "learning_rate = 0.0001\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1)\n",
        "valid_loss_min = np.Inf\n",
        "counter = 0\n",
        "\n",
        "n_iters = 1000\n",
        "num_epochs = n_iters / (len(X_train_scaled_m) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "for epochs in range(num_epochs):\n",
        "    model.train()\n",
        "    counter += 1\n",
        "    for i, (data, labels, mark) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        labels = labels.float().to(device).unsqueeze(2)\n",
        "\n",
        "        mark = mark.float().to(device)\n",
        "        # decoder input\n",
        "        dec_inp = torch.zeros_like(labels[:, -1:, :]).float()\n",
        "        dec_inp = torch.cat([labels[:, :4, :], dec_inp], dim=1).float().to(device)\n",
        "\n",
        "        outputs = model(data, mark, dec_inp, mark)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    val_losses = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data, labels, mark in valid_loader:\n",
        "                data = data.to(device)\n",
        "                labels = labels.float().unsqueeze(2).to(device)\n",
        "\n",
        "                mark = mark.float().to(device)\n",
        "                # decoder input\n",
        "                dec_inp = torch.zeros_like(labels[:, -1:, :]).float()\n",
        "                dec_inp = torch.cat([labels[:, :4, :], dec_inp], dim=1).float().to(device)\n",
        "\n",
        "                outputs = model(data, mark, dec_inp, mark)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_losses.append(loss.item())\n",
        "\n",
        "    model.train()\n",
        "    print(\"Epoch: {}/{}...\".format(epochs+1, num_epochs),\n",
        "          \"Loss: {:.6f}...\".format(loss.item()),\n",
        "          \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
        "\n",
        "    if np.mean(val_losses) < valid_loss_min:\n",
        "                torch.save(model.state_dict(), './state_dict_autoformer_m.pt')\n",
        "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
        "                valid_loss_min = np.mean(val_losses)\n",
        "                counter = 0\n",
        "    elif np.mean(val_losses) >= valid_loss_min and counter == 20:\n",
        "      break\n",
        "    #scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XllVf2NxRc4S",
        "outputId": "5d0f1d10-a26e-4646-d832-24739010adea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/172... Loss: 0.088105... Val Loss: 0.087457\n",
            "Validation loss decreased (inf --> 0.087457).  Saving model ...\n",
            "Epoch: 2/172... Loss: 0.085148... Val Loss: 0.075473\n",
            "Validation loss decreased (0.087457 --> 0.075473).  Saving model ...\n",
            "Epoch: 3/172... Loss: 0.090787... Val Loss: 0.076153\n",
            "Epoch: 4/172... Loss: 0.106779... Val Loss: 0.091388\n",
            "Epoch: 5/172... Loss: 0.131557... Val Loss: 0.114885\n",
            "Epoch: 6/172... Loss: 0.166249... Val Loss: 0.144035\n",
            "Epoch: 7/172... Loss: 0.201849... Val Loss: 0.170419\n",
            "Epoch: 8/172... Loss: 0.226451... Val Loss: 0.189249\n",
            "Epoch: 9/172... Loss: 0.240868... Val Loss: 0.201177\n",
            "Epoch: 10/172... Loss: 0.237544... Val Loss: 0.196842\n",
            "Epoch: 11/172... Loss: 0.222952... Val Loss: 0.184745\n",
            "Epoch: 12/172... Loss: 0.209092... Val Loss: 0.173906\n",
            "Epoch: 13/172... Loss: 0.195770... Val Loss: 0.163282\n",
            "Epoch: 14/172... Loss: 0.185494... Val Loss: 0.156075\n",
            "Epoch: 15/172... Loss: 0.188609... Val Loss: 0.158905\n",
            "Epoch: 16/172... Loss: 0.182560... Val Loss: 0.146211\n",
            "Epoch: 17/172... Loss: 0.175900... Val Loss: 0.139189\n",
            "Epoch: 18/172... Loss: 0.172182... Val Loss: 0.135018\n",
            "Epoch: 19/172... Loss: 0.170204... Val Loss: 0.132948\n",
            "Epoch: 20/172... Loss: 0.161278... Val Loss: 0.125118\n",
            "Epoch: 21/172... Loss: 0.152206... Val Loss: 0.121342\n",
            "Epoch: 22/172... Loss: 0.144472... Val Loss: 0.116617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate the prediciton output(exclude Autofomer)\n",
        "model.load_state_dict(torch.load('./state_dict_biLSTM-m.pt'))\n",
        "\n",
        "model.eval()\n",
        "X_test = X_test_scaled_m.to(device)\n",
        "Y_test = Y_test_m.to(device)\n",
        "output = model(X_test)\n",
        "output = output.detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "ye3Nxo8YJAC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate the prediciton output for Autoformer\n",
        "model.load_state_dict(torch.load('./state_dict_autoformer_m.pt'))\n",
        "\n",
        "model.eval()\n",
        "X_test = X_test_scaled_m.to(device)\n",
        "#Y_test = Y_test.to(device)\n",
        "Y_test = Y_test_m.to(device).unsqueeze(2)\n",
        "mark = X_test_mark.float().to(device)\n",
        "#decoder input\n",
        "dec_inp = torch.zeros_like(Y_test[:, -1:, :]).float()\n",
        "dec_inp = torch.cat([Y_test[:, :4, :], dec_inp], dim=1).float().to(device)\n",
        "\n",
        "output = model(X_test, mark, dec_inp, mark)\n",
        "#output = model(X_test)\n",
        "output = output.squeeze(2).detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "G77EMDmeRpM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this code use to generate the prediction for the training interval(exclude Autoformer)\n",
        "model.load_state_dict(torch.load('./state_dict_biLSTM-m.pt'))\n",
        "\n",
        "model.eval()\n",
        "X_train = X_train_scaled_m.to(device)\n",
        "output = model(X_train)\n",
        "output = output.detach().cpu().numpy()\n",
        "Y_train_m = Y_train_m.detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "IHdKWu8PLzmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this code use to generate the prediction for the training interval for Autoformer\n",
        "model.load_state_dict(torch.load('./state_dict_autoformer_m.pt'))\n",
        "model.eval()\n",
        "X_train = X_train_scaled_m.to(device)\n",
        "Y_train = Y_train_m.to(device).unsqueeze(2)\n",
        "mark = X_train_mark.float().to(device)\n",
        "dec_inp = torch.zeros_like(Y_train[:, -1:, :]).float()\n",
        "dec_inp = torch.cat([Y_train[:, :4, :], dec_inp], dim=1).float().to(device)\n",
        "\n",
        "output = model(X_train, mark, dec_inp, mark)\n",
        "output = output.squeeze(2).detach().cpu().numpy()\n",
        "Y_train = Y_train.squeeze(2).detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "UfeLAVh0Rxuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the training error\n",
        "error_var = (1/57) * np.sum((Y_train-output)**2)"
      ],
      "metadata": {
        "id": "UNV9SL2BMI6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoformer_m = pd.DataFrame(output, columns = ['s'])"
      ],
      "metadata": {
        "id": "QNEpwLL8Hrlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoformer_m['s5'] = output"
      ],
      "metadata": {
        "id": "m_n6ryUHHxPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoformer_m['average'] = autoformer_m.mean(axis=1)"
      ],
      "metadata": {
        "id": "cUajxDSULFye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "var = 0\n",
        "for i in range(5):\n",
        "  var += (autoformer_m.iloc[:,i].values - autoformer_m['average'].values[:,]) ** 2\n",
        "\n",
        "autoformer_m['k_var'] =  1/4 * var"
      ],
      "metadata": {
        "id": "Q-Yjch_LLOXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate the prediction interval\n",
        "autoformer_m['lower_bound'] = autoformer_m['average'].values + 1.96 * np.sqrt(autoformer_m['k_var'].values + error_var)\n",
        "autoformer_m['upper_bound'] = autoformer_m['average'].values - 1.96 * np.sqrt(autoformer_m['k_var'].values + error_var)"
      ],
      "metadata": {
        "id": "Z-xZeySzOo7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoformer_m.to_csv('autoformer_m_2.csv', index=False)"
      ],
      "metadata": {
        "id": "tirVod4EVypv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Total"
      ],
      "metadata": {
        "id": "m45g8pznY3aE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the dataloader for total (standardized)\n",
        "X_train_t = train_t.iloc[5:,3:]\n",
        "Y_train_t = torch.tensor(train_t['K2'][5:].values).to(torch.float32).unsqueeze(1)\n",
        "\n",
        "X_valid_t = valid_t.iloc[:,3:]\n",
        "Y_valid_t = torch.tensor(valid_t['K2'].values).to(torch.float32).unsqueeze(1)\n",
        "\n",
        "X_test_t = test_t.iloc[:,3:]\n",
        "Y_test_t = torch.tensor(test_t['K2'].values).to(torch.float32).unsqueeze(1)\n",
        "\n",
        "#X_hyper_t = t_params.iloc[5:,5:]\n",
        "#Y_hyper_t = torch.tensor(t_params['K2'][5:].values).to(torch.float32).unsqueeze(1)"
      ],
      "metadata": {
        "id": "QAhoUxuZY2Im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the scaler to your training data\n",
        "scaler.fit(X_train_t)\n",
        "X_train_scaled_t = torch.tensor(scaler.transform(X_train_t)).to(torch.float32).unsqueeze(2)\n",
        "X_valid_scaled_t = torch.tensor(scaler.transform(X_valid_t)).to(torch.float32).unsqueeze(2)\n",
        "X_test_scaled_t = torch.tensor(scaler.transform(X_test_t)).to(torch.float32).unsqueeze(2)\n",
        "#X_hyper_scaled_t = torch.tensor(scaler.transform(X_hyper_t)).to(torch.float32).unsqueeze(2)\n",
        "\n",
        "batch_size = 10\n",
        "\n",
        "\n",
        "#train = TensorDataset(X_train_scaled_t,Y_train_t)\n",
        "train = TensorDataset(X_train_scaled_t, Y_train_t, X_train_mark) # the X_train_mark is same as the mark generate in Female part\n",
        "train_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "#valid = TensorDataset(X_valid_scaled_t,Y_valid_t)\n",
        "valid = TensorDataset(X_valid_scaled_t, Y_valid_t, X_valid_mark) # the X_valid_mark is same as the mark generate in Female part\n",
        "valid_loader = DataLoader(valid, batch_size = 5, shuffle = False)"
      ],
      "metadata": {
        "id": "GAozu41_ZH8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameter tuning\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import torch.optim as optim\n",
        "from skorch import NeuralNetRegressor\n",
        "\n",
        "model = NeuralNetRegressor(LTSF_N,\n",
        "    criterion = nn.MSELoss,\n",
        "    optimizer = optim.Adam,\n",
        "    batch_size = 10,\n",
        "    max_epochs = 100,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "param_grid = {\n",
        "    'optimizer__lr': [0.001, 0.01, 0.1],\n",
        "    'module__activation':[nn.ReLU, nn.Tanh]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1,cv=5)\n",
        "\n",
        "\n",
        "grid_result = grid.fit(X_hyper_scaled_t,Y_hyper_t)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJiYu4gpZW78",
        "outputId": "e46b4c85-2bc9-443c-bdf1-35159724b4d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best: -1615.222297 using {'module__activation': <class 'torch.nn.modules.activation.ReLU'>, 'optimizer__lr': 0.1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # the traing process (exclude Autoformer)\n",
        "import torch.optim as optim\n",
        "\n",
        "model = biLSTM(30, 3, 0.1, nn.Tanh).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "learning_rate = 0.1\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "valid_loss_min = np.Inf\n",
        "counter = 0\n",
        "\n",
        "n_iters = 5000\n",
        "num_epochs = n_iters / (len(X_train_t) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "for epochs in range(num_epochs):\n",
        "    model.train()\n",
        "    for i, (data, labels) in enumerate(train_loader):\n",
        "        counter += 1\n",
        "        data = data.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if counter % 50 == 0:\n",
        "            val_losses = []\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "               for data, labels in valid_loader:\n",
        "                 data = data.to(device)\n",
        "                 labels = labels.to(device)\n",
        "                 outputs = model(data)\n",
        "                 loss = criterion(outputs, labels)\n",
        "                 val_losses.append(loss.item())\n",
        "\n",
        "            model.train()\n",
        "            print(\"Epoch: {}/{}...\".format(epochs+1, num_epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
        "\n",
        "            if np.mean(val_losses) < valid_loss_min:\n",
        "                torch.save(model.state_dict(), './state_dict_biLSTM-t.pt')\n",
        "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
        "                valid_loss_min = np.mean(val_losses)"
      ],
      "metadata": {
        "id": "y90_9SPmagv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the training process for Autoformer\n",
        "import torch.optim as optim\n",
        "model = Autoformer(256,4,128).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "learning_rate = 0.0001\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "#scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1)\n",
        "valid_loss_min = np.Inf\n",
        "counter = 0\n",
        "\n",
        "n_iters = 1000\n",
        "num_epochs = n_iters / (len(X_train_scaled_t) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "for epochs in range(num_epochs):\n",
        "    model.train()\n",
        "    counter += 1\n",
        "    for i, (data, labels, mark) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        labels = labels.float().to(device).unsqueeze(2)\n",
        "\n",
        "        mark = mark.float().to(device)\n",
        "        # decoder input\n",
        "        dec_inp = torch.zeros_like(labels[:, -1:, :]).float()\n",
        "        dec_inp = torch.cat([labels[:, :4, :], dec_inp], dim=1).float().to(device)\n",
        "\n",
        "        outputs = model(data, mark, dec_inp, mark)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    val_losses = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data, labels, mark in valid_loader:\n",
        "                data = data.to(device)\n",
        "                labels = labels.float().unsqueeze(2).to(device)\n",
        "\n",
        "                mark = mark.float().to(device)\n",
        "                # decoder input\n",
        "                dec_inp = torch.zeros_like(labels[:, -1:, :]).float()\n",
        "                dec_inp = torch.cat([labels[:, :4, :], dec_inp], dim=1).float().to(device)\n",
        "\n",
        "                outputs = model(data, mark, dec_inp, mark)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_losses.append(loss.item())\n",
        "\n",
        "    model.train()\n",
        "    print(\"Epoch: {}/{}...\".format(epochs+1, num_epochs),\n",
        "          \"Loss: {:.6f}...\".format(loss.item()),\n",
        "          \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
        "\n",
        "    if np.mean(val_losses) < valid_loss_min:\n",
        "                torch.save(model.state_dict(), './state_dict_autoformer_t.pt')\n",
        "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
        "                valid_loss_min = np.mean(val_losses)\n",
        "                counter = 0\n",
        "    elif np.mean(val_losses) >= valid_loss_min and counter == 20:\n",
        "      break\n",
        "    #scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtEn6XDqXBfh",
        "outputId": "88b69b66-72d5-4088-c1b9-a352793b9025"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/172... Loss: 0.693696... Val Loss: 0.710832\n",
            "Validation loss decreased (inf --> 0.710832).  Saving model ...\n",
            "Epoch: 2/172... Loss: 0.623723... Val Loss: 0.693104\n",
            "Validation loss decreased (0.710832 --> 0.693104).  Saving model ...\n",
            "Epoch: 3/172... Loss: 0.585070... Val Loss: 0.676705\n",
            "Validation loss decreased (0.693104 --> 0.676705).  Saving model ...\n",
            "Epoch: 4/172... Loss: 0.587045... Val Loss: 0.673341\n",
            "Validation loss decreased (0.676705 --> 0.673341).  Saving model ...\n",
            "Epoch: 5/172... Loss: 0.596236... Val Loss: 0.673470\n",
            "Epoch: 6/172... Loss: 0.611472... Val Loss: 0.671064\n",
            "Validation loss decreased (0.673341 --> 0.671064).  Saving model ...\n",
            "Epoch: 7/172... Loss: 0.623266... Val Loss: 0.665524\n",
            "Validation loss decreased (0.671064 --> 0.665524).  Saving model ...\n",
            "Epoch: 8/172... Loss: 0.670074... Val Loss: 0.648882\n",
            "Validation loss decreased (0.665524 --> 0.648882).  Saving model ...\n",
            "Epoch: 9/172... Loss: 0.714087... Val Loss: 0.641856\n",
            "Validation loss decreased (0.648882 --> 0.641856).  Saving model ...\n",
            "Epoch: 10/172... Loss: 0.751489... Val Loss: 0.626799\n",
            "Validation loss decreased (0.641856 --> 0.626799).  Saving model ...\n",
            "Epoch: 11/172... Loss: 0.745819... Val Loss: 0.608105\n",
            "Validation loss decreased (0.626799 --> 0.608105).  Saving model ...\n",
            "Epoch: 12/172... Loss: 0.727659... Val Loss: 0.589860\n",
            "Validation loss decreased (0.608105 --> 0.589860).  Saving model ...\n",
            "Epoch: 13/172... Loss: 0.694547... Val Loss: 0.580216\n",
            "Validation loss decreased (0.589860 --> 0.580216).  Saving model ...\n",
            "Epoch: 14/172... Loss: 0.592320... Val Loss: 0.559158\n",
            "Validation loss decreased (0.580216 --> 0.559158).  Saving model ...\n",
            "Epoch: 15/172... Loss: 0.476537... Val Loss: 0.538299\n",
            "Validation loss decreased (0.559158 --> 0.538299).  Saving model ...\n",
            "Epoch: 16/172... Loss: 0.408945... Val Loss: 0.510413\n",
            "Validation loss decreased (0.538299 --> 0.510413).  Saving model ...\n",
            "Epoch: 17/172... Loss: 0.314956... Val Loss: 0.476493\n",
            "Validation loss decreased (0.510413 --> 0.476493).  Saving model ...\n",
            "Epoch: 18/172... Loss: 0.240039... Val Loss: 0.451511\n",
            "Validation loss decreased (0.476493 --> 0.451511).  Saving model ...\n",
            "Epoch: 19/172... Loss: 0.251486... Val Loss: 0.435572\n",
            "Validation loss decreased (0.451511 --> 0.435572).  Saving model ...\n",
            "Epoch: 20/172... Loss: 0.257125... Val Loss: 0.404516\n",
            "Validation loss decreased (0.435572 --> 0.404516).  Saving model ...\n",
            "Epoch: 21/172... Loss: 0.250525... Val Loss: 0.367250\n",
            "Validation loss decreased (0.404516 --> 0.367250).  Saving model ...\n",
            "Epoch: 22/172... Loss: 0.157599... Val Loss: 0.296227\n",
            "Validation loss decreased (0.367250 --> 0.296227).  Saving model ...\n",
            "Epoch: 23/172... Loss: 0.021321... Val Loss: 0.201705\n",
            "Validation loss decreased (0.296227 --> 0.201705).  Saving model ...\n",
            "Epoch: 24/172... Loss: 0.011948... Val Loss: 0.142523\n",
            "Validation loss decreased (0.201705 --> 0.142523).  Saving model ...\n",
            "Epoch: 25/172... Loss: 0.047011... Val Loss: 0.101271\n",
            "Validation loss decreased (0.142523 --> 0.101271).  Saving model ...\n",
            "Epoch: 26/172... Loss: 0.027868... Val Loss: 0.038402\n",
            "Validation loss decreased (0.101271 --> 0.038402).  Saving model ...\n",
            "Epoch: 27/172... Loss: 0.000016... Val Loss: 0.008717\n",
            "Validation loss decreased (0.038402 --> 0.008717).  Saving model ...\n",
            "Epoch: 28/172... Loss: 0.016349... Val Loss: 0.049199\n",
            "Epoch: 29/172... Loss: 0.014447... Val Loss: 0.086484\n",
            "Epoch: 30/172... Loss: 0.004210... Val Loss: 0.075951\n",
            "Epoch: 31/172... Loss: 0.123597... Val Loss: 0.083276\n",
            "Epoch: 32/172... Loss: 0.246243... Val Loss: 0.118326\n",
            "Epoch: 33/172... Loss: 0.262782... Val Loss: 0.118860\n",
            "Epoch: 34/172... Loss: 0.232690... Val Loss: 0.114289\n",
            "Epoch: 35/172... Loss: 0.274114... Val Loss: 0.160784\n",
            "Epoch: 36/172... Loss: 0.448284... Val Loss: 0.242047\n",
            "Epoch: 37/172... Loss: 0.491044... Val Loss: 0.247763\n",
            "Epoch: 38/172... Loss: 0.424266... Val Loss: 0.202873\n",
            "Epoch: 39/172... Loss: 0.324846... Val Loss: 0.172364\n",
            "Epoch: 40/172... Loss: 0.419906... Val Loss: 0.221037\n",
            "Epoch: 41/172... Loss: 0.418020... Val Loss: 0.216139\n",
            "Epoch: 42/172... Loss: 0.424380... Val Loss: 0.203009\n",
            "Epoch: 43/172... Loss: 0.372839... Val Loss: 0.179598\n",
            "Epoch: 44/172... Loss: 0.381759... Val Loss: 0.182219\n",
            "Epoch: 45/172... Loss: 0.414679... Val Loss: 0.192761\n",
            "Epoch: 46/172... Loss: 0.333651... Val Loss: 0.181256\n",
            "Epoch: 47/172... Loss: 0.270386... Val Loss: 0.177728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate the prediciton output(exclude Autofomer)\n",
        "model.load_state_dict(torch.load('./state_dict_biLSTM-t.pt'))\n",
        "\n",
        "model.eval()\n",
        "X_test = X_test_scaled_t.to(device)\n",
        "Y_test = Y_test_t.to(device)\n",
        "output = model(X_test)\n",
        "output = output.detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "sNvxrYNmbjV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate the prediciton output for Autoformer\n",
        "model.load_state_dict(torch.load('./state_dict_autoformer_t.pt'))\n",
        "\n",
        "model.eval()\n",
        "X_test = X_test_scaled_t.to(device)\n",
        "#Y_test = Y_test.to(device)\n",
        "Y_test = Y_test_t.to(device).unsqueeze(2)\n",
        "mark = X_test_mark.float().to(device)\n",
        "#decoder input\n",
        "dec_inp = torch.zeros_like(Y_test[:, -1:, :]).float()\n",
        "dec_inp = torch.cat([Y_test[:, :4, :], dec_inp], dim=1).float().to(device)\n",
        "\n",
        "output = model(X_test, mark, dec_inp, mark)\n",
        "#output = model(X_test)\n",
        "output = output.squeeze(2).detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "oC2voPz2XUjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this code use to generate the prediction for the training interval(exclude Autoformer)\n",
        "model.load_state_dict(torch.load('./state_dict_biLSTM-t.pt'))\n",
        "\n",
        "model.eval()\n",
        "X_train = X_train_scaled_t.to(device)\n",
        "output = model(X_train)\n",
        "output = output.detach().cpu().numpy()\n",
        "Y_train_t = Y_train_t.detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "14SD2FJbkZAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this code use to generate the prediction for the training interval for Autoformer\n",
        "model.load_state_dict(torch.load('./state_dict_autoformer_t.pt'))\n",
        "model.eval()\n",
        "X_train = X_train_scaled_t.to(device)\n",
        "Y_train = Y_train_t.to(device).unsqueeze(2)\n",
        "mark = X_train_mark.float().to(device)\n",
        "dec_inp = torch.zeros_like(Y_train[:, -1:, :]).float()\n",
        "dec_inp = torch.cat([Y_train[:, :4, :], dec_inp], dim=1).float().to(device)\n",
        "\n",
        "output = model(X_train, mark, dec_inp, mark)\n",
        "output = output.squeeze(2).detach().cpu().numpy()\n",
        "Y_train = Y_train.squeeze(2).detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "cX2QDN2LXaWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate the traing error\n",
        "error_var = (1/57) * np.sum((Y_train-output)**2)"
      ],
      "metadata": {
        "id": "TNfM6aWylAtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoformer_t = pd.DataFrame(output, columns = ['s'])"
      ],
      "metadata": {
        "id": "OqshrSnXicP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoformer_t['s5'] = output"
      ],
      "metadata": {
        "id": "YaDFuIrXivfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoformer_t['average'] = autoformer_t.mean(axis=1)"
      ],
      "metadata": {
        "id": "Cuv5i1cYjvCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "var = 0\n",
        "for i in range(6):\n",
        "  var += (autoformer_t.iloc[:,i].values - autoformer_t['average'].values[:,]) ** 2\n",
        "\n",
        "autoformer_t['k_var'] =  1/5 * var"
      ],
      "metadata": {
        "id": "YKo2ADNVkB_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate the prediction interval\n",
        "autoformer_t['lower_bound'] = autoformer_t['average'].values + 1.96 * np.sqrt(autoformer_t['k_var'].values + error_var)\n",
        "autoformer_t['upper_bound'] = autoformer_t['average'].values - 1.96 * np.sqrt(autoformer_t['k_var'].values + error_var)"
      ],
      "metadata": {
        "id": "Rvec6O_2lJlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoformer_t.to_csv('autoformer_t_2.csv', index=False)"
      ],
      "metadata": {
        "id": "QN7cvN_7wV5B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}